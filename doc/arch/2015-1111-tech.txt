
双十一背后的技术力量

arch summit  架构峰会



1号店11.11：从应用架构落地点谈高可用高并发高性能
手机淘宝 521 性能优化项目揭秘
京东商品搜索系统架构设计
当当促销系统与交易系统的重构实践
苏宁系统拆分的一些经验谈
蘑菇街私有云平台的Docker应用实践
唯品会峰值系统应对实践
支付宝和蚂蚁花呗的技术架构及实践


天猫 京东 蘑菇街

交易额、交易峰值、支付峰值

高可用高并发高性能
分布式、高可扩展




///////////////// 1号店11.11：从应用架构落地点谈高可用高并发高性能



能落地的架构才是好架构，当然我们更缺的是好架构的落地点

1号店应用架构的演进大致经历了以下历程：
强依赖-> Service化->业务解耦->读写分离->异步->水平/垂直拆分->服务逻辑分组等。

SOA 服务化  

 监控和问题定位   日志

下单接口业务性强，可用、并发、性能要求

错误码 规范

RPC框架

Hessian   


业务垂直拆分
产品 用户 订单

文描、价格、库存、下单、订单查询等垂直服务

读写分离 
    订单主库 下单 订单更新和实时查询
    订单备库 非实时查询  离线计算

水平拆分 oracle迁 mysql


SOA中间件-Hedwig


服务自动注册发现、软负载均衡、节点踢出与复活、服务动态逻辑分组、请求自动重试等功能

并行请求、灰度发布 

调用链路及层次关系、日志分析、监控预警等

Haproxy  到 Hedwig 演进

apps > haproxy  >  services
apps > Hedwig > services


技术层面  读写分离、水平拆分、逻辑分组
业务流程的梳理、优化、改造   业务架构

业务解耦/异步

缓存  批量操作

业务梳理 业务场景细分
核心业务 与 非核心业务

异步本身不是什么高深的技术，关键是哪些业务可以走异步，这更体现架构师的业务理解能力和 综合能力。

下单成功后给用户的消息通知、发票详细信息的生成等都可以异步，
在不牺牲用户体验又保证业务流程完整的情况下，尽量走异步解耦，这对可用、性能都是极大的提升。


一个下单接口定义了135个错误编码


错误出现一个解决一个，系统自然越来越健壮和稳定

特价商品已售完无库存等

Service服务可用性99.9999%  6个9


销售库存准确率 超卖率

业务监控   订单监控

从实际订单数据和Service接口调用量两个维度去做监控，
保证了监控系统的稳定和准确（监控系统也会出错的），
其中下单接口调用量使用的就是Service日志数据。


服务监控  实时调用量 依赖


///////////////////
手机淘宝 521 性能优化项目揭秘


“1S法则”是面向Web侧，H5链路上加载性能和体验方向上的一个指标，具体指：
1)“强网”(4G/WIFI)下，1秒完全完成页面加载，包括首屏资源，可看亦可用;
2)3G下1秒完成首包的返回; 
3)2G下1秒完成建连。

优化措施
1.网络节点：HttpDNS优化
2.建连复用：SSL化，SPDY建连高复用
3.容器层面：离线化和预加载方案
4.前端组件：请求控制，域名收敛，图片库，

wifi 4G
2G/3G网络
建立连接耗时  

DNS UDP协议

 域名劫持
手淘HttpDNS服务在启动的时候就会对白名单的域名进行域名解析,返回对应服务的最近IP(各运营商),端口号,协议类型,心跳等信息。

CDN域名 
对域名解析而言，尤其是CDN域名，解析得到的IP应该更靠近客户端的地区和运营商，这样才能有更快的网络访问速度。
然而，由于运营商策略的多样性，其推送的Local DNS可能和客户端不在同一个地区，这时得到的解析结果可能不是最优的。
HttpDNS能够得到客户端的出口网关IP，从而能够更准确地判断客户端的地区和运营商，得到更精准的解析结果。


HttpDNS客户端SDK有几个特性：预解析、多域名解析 、TTL缓存和异步请求

域名收敛

1.图片资源域名全部收敛到gw.alicdn.com
2.前端图片库根据强弱网和设备分辨率做适配
3 首屏数据合并请求为一个。


APP几个常见的性能问题：
启动速度慢；界面跳转慢；事件响应慢；滑动和动画卡顿。

推送到达率（用户是否在线，用户所在网络质量）

推拉结合 增量更新

业务Bundle
初始化任务 

任务必要性  网络、主容器
任务独立性  基础库、中间件、业务
任务可并行
任务的异步串行
任务可插拔
任务可配置

检测超时方法，优化主线程
多线程治理  控制线程池的并发数和优先级
减少IO读写 
对于频繁读写数据库和SharedPreference以及文件的模块，通过增加缓存和降低采样率等手段减少对IO的读写

对于SharedPreference进行了专门的优化，减少单个文件的大小，将毫无联系的存储键值分开到不同文件中，并且防止将大数据块存储到SharedPreference中

降级部分功能

冷启动 热启动

界面优化

布局复杂、过渡绘制多、Activity主要函数耗时、内容展示慢、界面重新布局（Layout）、GC次数多

优化GPU的过渡绘制

去掉层叠布局中多余的背景设置、图片控件有前景内容的时候不显示背景、界面背景定义到Activity的主题中、减少Drawable的复杂Shape使用

优化层级和布局

删除无用的层级 用Merge标签或者ViewStub标签

优化动画细节 移出可视区域后停止动画轮播
阻断多余requestLayout
减少GC次数
谁占用了内存

Bitmap、BitmapDrawable


Ashmem即匿名共享内存
 Native Heap 
 
图片空间如何才能使用Ashmem，答案在Facebook推出的Fresco中已有提及，
那就是解码时的purgeable标记，这样在系统底层解码位图时会走Ashmem空间分配，而非Dalvik Heap空间。

BitmapFactory.Options options = new BitmapFactory.Options();
/*
* inPurgeable can help avoid big Dalvik heap allocations (from API level 11 onward)
 */
options.inPurgeable = true;
Bitmap bitmap = BitmapFactory.decodeByteArray(inputByteArray, 0, inputLength, options);


1.依据图片宽高生成空JPEG。
2.走系统解码接口完成Ashmem Bitmap生成。
3.覆写Pixel Data地址在libwebp完成解码。

MemoryFile



////////////////////
京东11.11：商品搜索系统架构设计


1. 海量的数据，亿级别的商品量；
2. 高并发查询，日PV过亿；
3. 请求需要快速响应。


1. 离线信息处理系统；
2. 索引系统；
3. 搜索服务系；
4.反馈和排序系统。


1. 商品数据已经结构化，但散布在商品、库存、价格、促销、仓储等多个系统；
2. 召回率要求高，保证每一个正常的商品均能够被搜索到；
3. 为保证用户体验，商品信息变更（比如价格、库存的变化）实时性要求高，导致更新量大，每天的更新量为千万级别；
4. 较强的个性化需求    筛选 排序


merger
query processor server  QP
detail server：搜索结果展示服务
全量增量索引
索引分片  请求结果合并排序

数据抽取 来自多个系统  宽表
倒排索引
搜索服务系统
数据分片

排序
反馈系统主要包含用户行为数据的实时收集、加工

弹性云 动态扩容

三级缓存 
 term的缓存，相关性计算缓存和翻页缓存
图像检索


/////////////////////
当当11.11：促销系统与交易系统的重构实践

基本信息 活动名称 时间
活动维度 地区 平台 用户等级 渠道 商品
促销条件 金额 数量 
促销优惠 送赠品 打N折 减N元 以N元销售
促销作用域 SKU SPU 品牌 品类 店铺 
促销规则属性 限购策略 库存策略 阶梯滚动

灰度发布 nginx  运行时reload配置


F5 > nginx集群 > server集群

线上测试 数据对比  测试白名单 黑名单

//////////////
苏宁11.11：系统拆分的一些经验谈


基础平台
基础架构方面包括自建CDN、云计算和云存储
通用系统方面包括短信、邮件、验证等系统
系统集成包方面括系统之间的通讯、统一验证和内部管理系统的统一权限
中间件方面包括Session共享、分布式调用链、Redis分片存储、数据库的分库分表中间件、统一配置管理和流控；
平台方面：运维监控平台，持续集成平台，大数据分析平台；
还有针对安全的风控系统等


////////////////
蘑菇街11.11：私有云平台的Docker应用实践


稳定性，容灾能力，紧急故障处理，应急处理  运维
私有云 
Docker  CaaS  Container as a Service
KVM IaaS
Docker 轻量化，秒级启动，标准化的打包／部署／运行的方案，镜像的快速分发，基于镜像的灰度发布
集群管理 OpenStack
Docker适合于无状态，分布式的业务，KVM适合对安全性，隔离性要求更高的业务
OpenStack＋novadocker＋Docker

编排调度 弹性伸缩 灰度升级

网络模式 NAT 
linux  bridge openvswitch
iptables

Kubernetes、Mesos、Hyper、criu、runC




////////////////
唯品会11.11：峰值系统应对实践

系统模块有效切分
服务化解耦，集中服务治理
增加异步访问
多阶段缓存，降低后端压力
优化数据库访问
加强系统监控

Venus是唯品会自开发的一款基于Spring的 Java开发框架

数据库访问层封装，支持分库、分表，连接池优化
缓存（Redis/Memcached）接口封装，连接池优化
CRUD服务代码自动生成（包含数据库操作）
OSP/REST服务调用接口封装及异步支持
ValidateInternals
单元／集成测试模版代码自动生成
配置中心


开放服务平台（OSP）的主要目标是提供服务化的核心远程调用机制
路由、负载均衡、服务保护和优雅降级
软负载   路由 灰度发布

限流 防刷 nginx lua 脚本插件，
服务降级

异步   
在用户下单后，其他系统如物流、供应商系统、配送、财务等系统需要获取订单详情、订单状态，
订单系统通过异步消息方式将订单的变化通知其它系统，异步调用实现系统间隔离解耦，
上下游系统业务逻辑分离，下游系统只需要解析异步消息进行处理，不需要依赖上游系统的业务逻辑，
从而降低了系统之间的依赖。即使下游系统出现异常，订单系统仍然能正常处理数据。

动静分离，静态化
用户浏览器缓存静态资源，失效时间通过cache-control来控制
CND
应用服务器本地缓存

系统/网络层面监控、应用层面监控和业务层面监控

系统/网络层面监控，主要是对下列指标进行
监控：服务器指标，如CPU、内存、磁盘、流量、TCP连接数等；数据库指标，如QPS、主从复制延时、进程、慢查询等。

业务监控 pv uv 订单数
应用监控 错误数 异常数 响应时间 调用量



////////////
蚂蚁金服11.11：支付宝和蚂蚁花呗的技术架构及实践

SAAS  PAAS  IAAS  

1.运维平台（IAAS）：主要提供基础资源的可伸缩性，比如网络、存储、数据库、虚拟化、IDC等，保证底层系统平台的稳定性；
2.技术平台（PAAS）：主要提供可伸缩、高可用的分布式事务处理和服务计算能力，
                    能够做到弹性资源的分配和访问控制，提供一套基础的中间件运行环境，屏蔽底层资源的复杂性；
3.业务平台（SAAS）：提供随时随地高可用的支付服务，并且提供一个安全易用的开放支付应用开发平台。


路由 
机房1 机房2 
单元1 单元2 
单元(应用 数据 网络 )


GSLB  Server Load Balancer DNS
IDC(Internet Data Center)
lvs spanner ssl
spanner ?

逻辑数据中心架构 ，核心思想是把数据水平拆分的思路向上层提到接入层、终端，
从接入层 开始把系统分成多个单元，单元有几个特性：

1.每个单元对外是封闭的，包括系统间交换各类存储的访问;
2.每个单元的实时数据是独立的，不共享。而会员或配置类对延时性要求不高的数据可共享;
3.单元之间的通信统一管控，尽量走异步化消息。同步消息走单元代理方案。

这套架构解决了几个关键问题：


1. 由于尽量减少了跨单元交互和使用异步化，使得异地部署成为可能。整个系统的水平可伸缩性大大提高，不再依赖同城IDC；

2. 可以实现N+1的异地灾备策略，大大缩减灾备成本，同时确保灾备设施真实可用；

3. 整个系统已无单点存在，大大提升了整体的高可用性；同城和异地部署的多个单元可用作互备的容灾设施，
   通过运维管控平台进行快速切换，有机会实现100%的持续可用率；

4. 该架构下业务级别的流量入口和出口形成了统一的可管控、可路由的控制点，
   整体系统的可管控能力得到很大提升。基于该架构，线上压测、流量管控、灰度发布等以前难以实现的运维管控模式，现在能够十分轻松地实现。



  异步化  异地部署 异地灾备 
  统一入口 出口    流控 灰度发布

异地多活

传统的 两地三中心  生产数据中心、同城灾备中心、异地灾备中心

逻辑机房 LDC

分布式数据架构
支付宝在2015年双十一当天的高峰期间处理支付峰值8.59万笔/秒，已经是国际第一大系统支付
支付宝数据架构发展，就是一部低成本、线性可伸缩、分布式的数据架构演变史


现在支付宝的数据架构已经从集中式的小型机和高端存储升级到了分布式PC服务解决方案，整体数据架构的解决方案尽量做到无厂商依赖，并且标准化。


垂直拆分 按业务类型拆分 
水平拆分 按客户/请求拆分

读写分离 数据复制 


交易(写) 交易读  


核心交易库  分表 分库 主备
交易记录库 分库分表 

分布式数据访问层
数据复制中心
ACID（原子性、一致性、隔离性、持久性
CAP BASE
分布式事务框架 支持两阶段提交协议 但做了优化  貌似没有准备阶段

柔性事务策略
















